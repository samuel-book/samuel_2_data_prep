{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean raw data\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook cleans the SAMueL-2 SAMueL SSNAP extract v2 (not included on GitHub repository). This data only contains patients with **stroke onset out of hospital** (all in-hospital strokes removed prior to receiving data). For an explanation of the decisions made when data cleaning (for example, why made changes to certain variables, or how decided to drop unusual results), please see \"02_reasoning_behind_clean_raw_data.ipynb\".\n",
    "\n",
    "## Aims\n",
    "\n",
    "(1) Extract and clean relevant variables from the raw SSNAP data - for example:\n",
    "* Converting from Y/N/NB to 0/1.\n",
    "* Converting categories to numbers (e.g. age 40-45 to 42.5).\n",
    "* Creating indicators (e.g. whether stroke onset time was known, by grouping \"precise\" with \"best estimate\", and comparing that to \"not known\")\n",
    "* Calculating times (e.g. comparing time arrived at location to time call connected, to determine call to ambulance arrival time).\n",
    "\n",
    "(2) Deal with unusual results (that apply for any project using this dataset) - for example:\n",
    "* Replacing values as missing when they are considered to be invalid/implausible/errors.\n",
    "* Removing patients from the dataset.\n",
    "\n",
    "Individual projects (i.e. different models or analyses) may impose additional restrictions on the dataset (e.g. dropping patients missing vital data, restricting date range)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Linting\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "\n",
    "# Set the maximum number of columns to 100\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and filenames\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "    data_path: str = './data'\n",
    "    data_filename: str = 'SAMueL ssnap extract v2.csv'\n",
    "    data_save_path: str = './output'\n",
    "    data_save_filename: str = 'clean_samuel_ssnap_extract_v2.csv'\n",
    "    database_filename: str = 'samuel.db'\n",
    "\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data: pd.DataFrame = pd.read_csv(\n",
    "    os.path.join(paths.data_path, paths.data_filename), low_memory=False)\n",
    "\n",
    "# Set up DataFrame for cleaned data\n",
    "cleaned_data: pd.DataFrame = pd.DataFrame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clean version of data\n",
    "\n",
    "### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['id'] = raw_data['PatientId']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['stroke_team'] = raw_data['TeamName']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary defining numeric age as middle of age band\n",
    "ages: dict = {'AgeUnder40': 37.5,\n",
    "              'Age40to44': 42.5, 'Age45to49': 47.5,\n",
    "              'Age50to54': 52.5, 'Age55to59': 57.5,\n",
    "              'Age60to64': 62.5, 'Age65to69': 67.5,\n",
    "              'Age70to74': 72.5, 'Age75to79': 77.5,\n",
    "              'Age80to84': 82.5, 'Age85to89': 87.5,\n",
    "              'AgeOver90': 92.5}\n",
    "\n",
    "# Extract age band columns, and find the highest age band that\n",
    "# # the patient is part of\n",
    "col_extract: pd.DataFrame = raw_data[ages.keys()]\n",
    "age_band: pd.Series = col_extract.idxmax(axis=1)\n",
    "\n",
    "# Use that ageband to find appropriate numeric age from ages dictionary\n",
    "cleaned_data['age'] = age_band.map(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender: dict = {'M': 1, 'F': 0}\n",
    "cleaned_data['male'] = raw_data['S1Gender'].map(gender)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke type\n",
    "\n",
    "Abbreviations:\n",
    "* Infarction (I)\n",
    "* Primary intracerebral haemorrage (PIH)\n",
    "* Unknown if not imaged (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "infarction: dict = {'I': 1, 'PIH': 0}\n",
    "cleaned_data['infarction'] = raw_data['S2StrokeType'].map(infarction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onset time\n",
    "\n",
    "Abbreviations:\n",
    "* Precise (P)\n",
    "* Best estimate (BE)\n",
    "* Not known (NK)\n",
    "* During sleep (DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onset to arrival time in minutes\n",
    "cleaned_data['onset_to_arrival_time'] = raw_data['OnsettoArrivalMinutes']\n",
    "\n",
    "# Whether onset time is known\n",
    "onset_known: dict = {'NK': 0, 'P': 1, 'BE': 1}\n",
    "cleaned_data['onset_known'] = raw_data['S1OnsetTimeType'].map(onset_known)\n",
    "\n",
    "# Whether onset time is precise - if not, then best estimate or not known\n",
    "precise_onset_known: dict = {'P': 1, 'BE': 0, 'NK': 0}\n",
    "cleaned_data['precise_onset_known'] = (\n",
    "    raw_data['S1OnsetTimeType'].map(precise_onset_known))\n",
    "\n",
    "# Stroke during sleep\n",
    "sleep: dict = {'DS': 1, 'P': 0, 'BE': 0}\n",
    "cleaned_data['onset_during_sleep'] = raw_data['S1OnsetDateType'].map(sleep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambulance timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrive by ambulance\n",
    "by_ambulance: dict = {'Y': 1, 'N': 0}\n",
    "cleaned_data['arrive_by_ambulance'] = (\n",
    "    raw_data['S1ArriveByAmbulance'].map(by_ambulance))\n",
    "\n",
    "# Time from call connected to ambulance arrival at patient location\n",
    "cleaned_data['call_to_ambulance_arrival_time'] = (\n",
    "    raw_data['ArrivalPatientLocationtoArrivalMinutes'] -\n",
    "    raw_data['CallConnectedtoArrivalMinutes'])\n",
    "\n",
    "# Time that ambulance on scene at patient location\n",
    "cleaned_data['ambulance_on_scene_time'] = (\n",
    "       raw_data['DeparturePatientLocationtoArrivalMinutes'] -\n",
    "       raw_data['ArrivalPatientLocationtoArrivalMinutes'])\n",
    "\n",
    "# Ambulance travel time to from patient location to hospital\n",
    "cleaned_data['ambulance_travel_to_hospital_time'] = (\n",
    "       raw_data['WheelsStoptoArrivalMinutes'] -\n",
    "       raw_data['DeparturePatientLocationtoArrivalMinutes'])\n",
    "\n",
    "# Ambulance wait time at hospital\n",
    "cleaned_data['ambulance_wait_time_at_hospital'] = (\n",
    "    0 - raw_data['WheelsStoptoArrivalMinutes'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day, month, year and time of arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month, year and day\n",
    "cleaned_data['month'] = pd.to_datetime(\n",
    "    raw_data['FirstArrivalMonthYear'], format='%b-%y').dt.month\n",
    "cleaned_data['year'] = pd.to_datetime(\n",
    "    raw_data['FirstArrivalMonthYear'], format='%b-%y').dt.year\n",
    "cleaned_data['weekday'] = raw_data['FirstArrivalWeekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrival period (3 hour period during day)\n",
    "arrival_time_dict: dict = {\n",
    "    '0000to3000': 0,\n",
    "    '0300to0600': 3,\n",
    "    '0600to0900': 6,\n",
    "    '0900to1200': 9,\n",
    "    '1200to1500': 12,\n",
    "    '1500to1800': 15,\n",
    "    '1800to2100': 18,\n",
    "    '2100to2400': 21\n",
    "}\n",
    "cleaned_data['arrival_time_3_hour_period'] = (\n",
    "    raw_data['FirstArrivalTime'].map(arrival_time_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan, thrombolysis and thrombectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrival to scan time\n",
    "cleaned_data['arrival_to_scan_time'] = raw_data['ArrivaltoBrainImagingMinutes']\n",
    "\n",
    "# Get use of thrombolysis\n",
    "# NB is the answer automatically selected if type of stroke is PIH\n",
    "thrombolysis: dict = {'Y': 1, 'N': 0, 'NB': 0}\n",
    "cleaned_data['thrombolysis'] = raw_data['S2Thrombolysis'].map(thrombolysis)\n",
    "\n",
    "# Get time from scan to thrombolysis\n",
    "cleaned_data['scan_to_thrombolysis_time'] = (\n",
    "    raw_data['ArrivaltoThrombolysisMinutes'] -\n",
    "    raw_data['ArrivaltoBrainImagingMinutes'])\n",
    "\n",
    "# Get use of thrombectomy (0 if x is NaN, 1 if x is a number)\n",
    "cleaned_data['thrombectomy'] = (\n",
    "    raw_data['ArrivaltoArterialPunctureMinutes'].apply(\n",
    "        lambda x: 0 if np.isnan(x) else 1))\n",
    "\n",
    "# Get time from arrival to thrombectomy\n",
    "cleaned_data['arrival_to_thrombectomy_time'] = (\n",
    "    raw_data['ArrivaltoArterialPunctureMinutes'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comorbidities\n",
    "\n",
    "These are co-morbidities that were present prior to this admission, and medication that patient was on prior to this admission. The one exception is S2NewAfDiagnosis, which is whether a new diagnosis of atrial fibrillation was made on admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities: dict = {\n",
    "    'S2CoMCongestiveHeartFailure': 'congestive_heart_failure',\n",
    "    'S2CoMHypertension': 'hypertension',\n",
    "    'S2CoMAtrialFibrillation': 'atrial_fibrillation',\n",
    "    'S2CoMDiabetes': 'diabetes',\n",
    "    'S2CoMStrokeTIA': 'prior_stroke_tia',\n",
    "    'S2CoMAFAntiplatelet': 'afib_antiplatelet',\n",
    "    'S2CoMAFAnticoagulent': 'afib_anticoagulant'}\n",
    "\n",
    "# Add comorbidites columns with new names and change Y/N/NB to 1/0\n",
    "cleaned_data[list(comorbidities.values())] = raw_data[comorbidities.keys()]\n",
    "comorbid_marker = {'Y': 1, 'N': 0, 'NB': 0, np.nan: np.nan}\n",
    "for col in comorbidities.values():\n",
    "    cleaned_data[col] = cleaned_data[col].map(comorbid_marker)\n",
    "\n",
    "# You cannot be marked as receiving antiplatelets unless you have an atrial\n",
    "# fibrillation diagnosis, so change those from missing to 0\n",
    "cleaned_data.loc[(cleaned_data['atrial_fibrillation'] == 0) &\n",
    "                 (cleaned_data['afib_antiplatelet'].isnull()),\n",
    "                 'afib_antiplatelet'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticoag_type: dict = {\n",
    "    'S2CoMAFAnticoagulentVitK': 'afib_vit_k_anticoagulant',\n",
    "    'S2CoMAFAnticoagulentDOAC': 'afib_doac_anticoagulant',\n",
    "    'S2CoMAFAnticoagulentHeparin': 'afib_heparin_anticoagulant'\n",
    "}\n",
    "\n",
    "# Add to clean data and map (seperately from other comorbidities as\n",
    "# dictionary states that leaving empty (NaN) means False for these 3)\n",
    "cleaned_data[list(anticoag_type.values())] = raw_data[anticoag_type.keys()]\n",
    "anticoag_type_map = {1: 1, 0: 0, np.nan: 0}\n",
    "for col in anticoag_type.values():\n",
    "    cleaned_data[col] = cleaned_data[col].map(anticoag_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_afib = {'Y': 1, 'N': 0, np.nan: np.nan}\n",
    "cleaned_data['new_afib_diagnosis'] = raw_data['S2NewAFDiagnosis'].map(new_afib)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['prior_disability'] = raw_data['S2RankinBeforeStroke']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIHSS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_to_snake(str):\n",
    "    # Converts CamelCase to snake_case\n",
    "    # Input: str\n",
    "    snake = ''.join(['_' + i.lower() if i.isupper()\n",
    "                     else i for i in str]).lstrip('_')\n",
    "    return snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke severity is NIHSS score on arrival\n",
    "cleaned_data['stroke_severity'] = raw_data['S2NihssArrival']\n",
    "\n",
    "# List of NIHSS arrival measures\n",
    "nihss: list = ['S2NihssArrivalLoc', 'S2NihssArrivalLocQuestions',\n",
    "               'S2NihssArrivalLocCommands', 'S2NihssArrivalBestGaze',\n",
    "               'S2NihssArrivalVisual', 'S2NihssArrivalFacialPalsy',\n",
    "               'S2NihssArrivalMotorArmLeft', 'S2NihssArrivalMotorArmRight',\n",
    "               'S2NihssArrivalMotorLegLeft', 'S2NihssArrivalMotorLegRight',\n",
    "               'S2NihssArrivalLimbAtaxia', 'S2NihssArrivalSensory',\n",
    "               'S2NihssArrivalBestLanguage', 'S2NihssArrivalDysarthria',\n",
    "               'S2NihssArrivalExtinctionInattention']\n",
    "\n",
    "# Finds the minimum value across these columns, and uses that to create\n",
    "# marker of whether any of them contain a missing value (indicated by -1)\n",
    "cleaned_data['nihss_complete'] = raw_data[nihss].min(axis=1).apply(\n",
    "    lambda x: 0 if x == -1 else 1)\n",
    "\n",
    "# Add columns (exactly as are in raw data)\n",
    "cleaned_data[nihss] = raw_data[nihss]\n",
    "\n",
    "# Rename - convert to snake case and remove 's2_'\n",
    "rename_dict: dict = {}\n",
    "for col in nihss:\n",
    "    rename_dict[col] = camel_to_snake(col).split('s2_')[1]\n",
    "cleaned_data.rename(rename_dict, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Death and outcome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discharge destination\n",
    "discharge: dict = {\n",
    "    'CH': 'care_home',\n",
    "    'D': 'died',\n",
    "    'H': 'home',\n",
    "    'SE': 'somewhere_else',\n",
    "    'TC': 'community_team_or_esd',\n",
    "    'TCN': 'community_team_or_esd',\n",
    "    'TN': 'non_ssnap_hospital_team',\n",
    "    'T': 'ssnap_hospital_team',\n",
    "    np.NaN: 'missing'}\n",
    "cleaned_data['discharge_destination'] = (\n",
    "    raw_data['S7DischargeType'].map(discharge))\n",
    "\n",
    "# Death - if NaN then 0, if 0+ days (so if died) then 1\n",
    "cleaned_data['death'] = (raw_data['ArrivalToDeathDays'] >= 0) * 1\n",
    "\n",
    "# Outcome\n",
    "cleaned_data['discharge_disability'] = raw_data['S7RankinDischarge']\n",
    "cleaned_data['disability_6_month'] = raw_data['S8Rankin6Month']\n",
    "# S8Rankin6MonthNK not included as implicit that not known\n",
    "# if NaN is S8Rankin6Month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for no thrombolysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"No\" reasons\n",
    "\n",
    "Abbreviations:\n",
    "* TNA - Thrombolysis not available at hospital at all\n",
    "* OTSH - Outside thrombolysis service hours\n",
    "* USQE - Unable to scan quickly enough\n",
    "* N - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_no_thrombolysis(category):\n",
    "    '''\n",
    "    Function used to create thrombolysis no variables.\n",
    "    Created using map rather than ifelse (which would require less code)\n",
    "    as map is explicitly stating what each variable is mapped to\n",
    "    (e.g. being aware that NaN is also mapping to 0)\n",
    "    '''\n",
    "    # Create dictionary mapping every category to 0\n",
    "    thrombolysis_no = {'TNA': 0, 'OTSH': 0, 'USQE': 0, 'N': 0, np.nan: 0}\n",
    "\n",
    "    # Change specific category to map to 1\n",
    "    thrombolysis_no[category] = 1\n",
    "\n",
    "    # Return thrombolysis no column mapped to this dictionary\n",
    "    return (raw_data['S2ThrombolysisNoReason'].map(thrombolysis_no))\n",
    "\n",
    "\n",
    "# Create columns with reason for no thrombolysis - if 0 for all then was NaN\n",
    "cleaned_data['thrombolysis_no_not_available'] = make_no_thrombolysis('TNA')\n",
    "cleaned_data['thrombolysis_no_out_of_hours'] = make_no_thrombolysis('OTSH')\n",
    "cleaned_data['thrombolysis_no_scan_not_quick_enough'] = (\n",
    "    make_no_thrombolysis('USQE'))\n",
    "cleaned_data['thrombolysis_no_no_reason'] = make_no_thrombolysis('N')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"No but\" reasons\n",
    "\n",
    "*Note: NoButTimeWindow refers to time since stroke onset, and not time window that thrombolysis services were available (that is covered in the \"no\" reasons)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_thrombolysis: list = [\n",
    "       'S2ThrombolysisNoButHaemorrhagic', 'S2ThrombolysisNoButTimeWindow',\n",
    "       'S2ThrombolysisNoButComorbidity', 'S2ThrombolysisNoButMedication',\n",
    "       'S2ThrombolysisNoButRefusal', 'S2ThrombolysisNoButAge',\n",
    "       'S2ThrombolysisNoButImproving', 'S2ThrombolysisNoButTooMildSevere',\n",
    "       'S2ThrombolysisNoButTimeUnknownWakeUp',\n",
    "       'S2ThrombolysisNoButOtherMedical']\n",
    "\n",
    "# Add columns\n",
    "cleaned_data[no_thrombolysis] = raw_data[no_thrombolysis]\n",
    "\n",
    "# Rename - convert to snake case and remove 's2_'\n",
    "rename_dict: dict = {}\n",
    "for col in no_thrombolysis:\n",
    "    rename_dict[col] = camel_to_snake(col).split('s2_')[1]\n",
    "cleaned_data.rename(rename_dict, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with unusual results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unusual ambulance times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_col = ['call_to_ambulance_arrival_time',\n",
    "           'ambulance_on_scene_time',\n",
    "           'ambulance_travel_to_hospital_time',\n",
    "           'ambulance_wait_time_at_hospital']\n",
    "\n",
    "amb_except_wait = ['call_to_ambulance_arrival_time',\n",
    "                   'ambulance_on_scene_time',\n",
    "                   'ambulance_travel_to_hospital_time']\n",
    "\n",
    "# If any times are missing, set all to NaN\n",
    "cleaned_data.loc[cleaned_data[amb_col].isnull().any(axis=1), amb_col] = np.nan\n",
    "\n",
    "# If any times are negative, set all to NaN\n",
    "cleaned_data.loc[(cleaned_data[amb_col] < 0).any(axis=1), amb_col] = np.nan\n",
    "\n",
    "# If any ambulance column except waittime is 0, set all to NaN\n",
    "cleaned_data.loc[\n",
    "    (cleaned_data[amb_except_wait] == 0).any(axis=1), amb_col] = np.nan\n",
    "\n",
    "# Find individuals with ambulance times yet arrive_by_ambulance = 0\n",
    "# For those individuals, set ambulance times + arrive by ambulance to NaN\n",
    "times_but_no_amb = (\n",
    "    cleaned_data['arrive_by_ambulance'] == 0) & (\n",
    "        (cleaned_data['call_to_ambulance_arrival_time'].notnull()) |\n",
    "        (cleaned_data['ambulance_on_scene_time'].notnull()) |\n",
    "        (cleaned_data['ambulance_travel_to_hospital_time'].notnull()) |\n",
    "        (cleaned_data['ambulance_wait_time_at_hospital'].notnull()))\n",
    "cleaned_data.loc[times_but_no_amb,\n",
    "                 amb_col + ['arrive_by_ambulance']] = np.nan\n",
    "\n",
    "# If call to ambulance arrive is greater than 24h (1440m), set all times to NaN\n",
    "cleaned_data.loc[cleaned_data['call_to_ambulance_arrival_time'] > 1440,\n",
    "                 amb_col] = np.nan\n",
    "\n",
    "# If ambulance on scene time is greater than 12h (720m), set all times to NaN\n",
    "cleaned_data.loc[cleaned_data['ambulance_on_scene_time'] > 720,\n",
    "                 amb_col] = np.nan\n",
    "\n",
    "# If travel to hospital time is greater than 6h (360m), set all times to NaN\n",
    "cleaned_data.loc[cleaned_data['ambulance_travel_to_hospital_time'] > 360,\n",
    "                 amb_col] = np.nan\n",
    "\n",
    "# If wait time at hospital is greater than 12h (720m), set all times to NaN\n",
    "cleaned_data.loc[cleaned_data['ambulance_wait_time_at_hospital'] > 720,\n",
    "                 amb_col] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unusual onset to arrival times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If onset to arrival = 0 or is negative, mark as missing\n",
    "cleaned_data.loc[(cleaned_data['onset_to_arrival_time'] <= 0),\n",
    "                 'onset_to_arrival_time'] = np.nan\n",
    "\n",
    "# If onset time is not known, set onset to arrival time as missing\n",
    "cleaned_data.loc[(cleaned_data['onset_known'] == 0),\n",
    "                 'onset_to_arrival_time'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inconsistent anticoagulant data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If discrepancy use of anticoagulants is negative, but an indivudal\n",
    "# anticoagulant is marked as being used, mark both as missing\n",
    "cleaned_data.loc[\n",
    "    ((cleaned_data['afib_anticoagulant'] == 0) &\n",
    "     ((cleaned_data['afib_vit_k_anticoagulant'] == 1) |\n",
    "      (cleaned_data['afib_doac_anticoagulant'] == 1) |\n",
    "      (cleaned_data['afib_heparin_anticoagulant'] == 1))),\n",
    "    ['afib_anticoagulant', 'afib_vit_k_anticoagulant',\n",
    "     'afib_doac_anticoagulant', 'afib_heparin_anticoagulant']] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inconsistent death markers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify patients who have any marker indicating death\n",
    "any_death = ((cleaned_data['death'] == 1) |\n",
    "             (cleaned_data['discharge_disability'] == 6) |\n",
    "             (cleaned_data['discharge_destination'] == 'died'))\n",
    "\n",
    "# Identify patients who have all markers indicating death\n",
    "# (or, if not, missing a result)\n",
    "all_or_missing_death = (\n",
    "    ((cleaned_data['death'] == 1) |\n",
    "     cleaned_data['death'].isnull()) &\n",
    "    ((cleaned_data['discharge_disability'] == 6) |\n",
    "     (cleaned_data['discharge_disability'].isnull())) &\n",
    "    ((cleaned_data['discharge_destination'] == 'died') |\n",
    "     (cleaned_data['discharge_destination'].isnull())))\n",
    "\n",
    "# Extract columns where have at least one marker indicating death,\n",
    "# but then others do not (don't extract if its just because others\n",
    "# are missing) - and set all their death columns as NaN\n",
    "cleaned_data.loc[\n",
    " any_death & ~all_or_missing_death,\n",
    " ['death', 'discharge_disability', 'discharge_destination']] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove patients\n",
    "\n",
    "This performed at the end of the notebook, as the data cleaning steps assume that the rows in the clean data match up to the rows in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove patients missing stroke type\n",
    "new_cleaned_data = cleaned_data[cleaned_data['infarction'].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(paths.data_save_path, paths.data_save_filename)\n",
    "new_cleaned_data.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
